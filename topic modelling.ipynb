{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing needed libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting text files\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "directory = \"/content/sample_data/test/\"\n",
    "\n",
    "\n",
    "txt_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "cnt =0\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        documents.append(content)\n",
    "        cnt +=1\n",
    "\n",
    "print(documents)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking data\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "data = pd.DataFrame({\"text\": documents})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing : tokenization, stopwords removal, lemmatization using spacy\n",
    "def preprocess(text):\n",
    "  doc = nlp(text)\n",
    "  processed_tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "  return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TF-IDF Vectorizer and fit the model\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#transform the doc to TF-IDF vectors\n",
    "X = vectorizer.fit_transform(data[\"processed_text\"])\n",
    "\n",
    "#create a latent Dirichlet Allocation model\n",
    "lda = LatentDirichletAllocation(n_components = cnt)\n",
    "\n",
    "#fit the model to the TF-IDF vectors\n",
    "lda.fit(X)\n",
    "\n",
    "#print the topics\n",
    "print(lda.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the topics and thier associated words\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "  print(f\"Topic {idx+1}: \")\n",
    "  #getting the top 5 words with highest weights for the topic\n",
    "  top_words_idx = topic.argsort()[-5:][::-1]\n",
    "  top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n",
    "  print(\", \".join(top_words))\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
